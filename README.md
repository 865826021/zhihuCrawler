## 知乎爬虫

----------

## 思路

首先是通过一个主文件传入操作类型，然后从配置文件里拿配置信息。

爬虫这一块，目前只是做了一个用户名获取的东西。

待到用户名多的时候就可以更好更快地拉取数据了。

#### 用户名获取

这是第一个表的内容

先在数据库写三五个种子用户名，然后通过数据库拿到这几个种子用户名，放入redis的链表结构中，

然后对redis的数据一个一个进行循环，每个用户名被拼接成`url`，随后curl就可以运行起来了。

获取到的页面信息传给分析器进行正则判断处理，随后发送回去。然后控制器把他们存入数据库。

当redis里的数据用完了就再从数据库拿数据。这时候数据库中就有上一轮存入的数据了。

如此循环，直到达到配置文件中要求。

（不考虑性能问题，捂脸，逃）


## 问题

#### 性能问题
首先就是性能问题，大概拿到8000条数据的时候服务器就宕机了。。。

是用`vagrant`起的服务器，可能性能不太好吧。

传言`php7`性能有两倍的提升，可能明天回去试一下

#### 编码问题

我对于算法不算很懂，不过好歹知道一些性能问题，能在内存里运行的运算就放内存里了。

之前学了一些设计模式，用起来简直不要太爽！


## 解决方案

首先是要上`php7`的，这个没得讲。

性能提升很明显了吧。opc应该也是要开。

另外，似乎不用特别着急在爬到用户名后立即存入数据库，可以的话就放到redis里面好了。等压力轻一点再存？

给`vagrant`更好一点儿的性能吧。

还有`swoole`，传说中性能非常棒。我主要看中了那个多线程模块。先学一下，后面给加上测试一下。

可能会放到我的`vps`上，不过那货性能也是一般。


## Todo

下一步先把另一个模块 -- **获取用户详细数据**的功能给完成了

然后考虑做性能上的提升，目标是100万数据量！加油啦！


## Usage

1. 你首先需要拉取代码：
```bash
https://github.com/AnnatarHe/zhihuCrawler.git
```

2. 运行`Database`里面的数据库创建的一些东西

3. 运行index.php
```bash
php index.php
```

4. 不出意外的话。。。你的服务器会挂掉:flushed:
